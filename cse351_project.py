# -*- coding: utf-8 -*-
"""CSE351 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q84dYjPyLPYAoO88cKvmujROI56SH3A8
"""

# Commented out IPython magic to ensure Python compatibility.
import io
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn.svm import SVC
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
# %matplotlib inline

df = pd.read_csv('train.csv') 
df2 = pd.read_csv('test.csv')

df.info()

"""In the train file, there are a lot of ages and cabin numbers missing"""

df2.info()

"""Similarily, the test.csv is also missing quite an amount of ages and cabin numbers."""

data_train = df.replace({'Sex':{'male':0, 'female':1}}) #changing the genders into numbers to make it easier for graphs
data_test = df2.replace({'Sex':{'male':0, 'female':1}}) #doing the same for the test
data_train['Embarked'] = data_train['Embarked'].fillna('S') #fill the NaN value in Embarked with 'S' since it aligns the most with the data
data_test['Embarked'] = data_test['Embarked'].fillna('S')
data_train = data_train.replace({'Embarked':{'C':1, 'Q':2, 'S':3}}) #changing embarked locations to numbers for easier graphs
data_test = data_test.replace({'Embarked':{'C':1, 'Q':2, 'S':3}}) #same for the test
data_train.pop('Ticket')  
data_test.pop('Ticket')
data_train.pop('Cabin')
data_test.pop('Cabin')
data_train.pop('Name')
data_test.pop('Name')
data_test.pop('PassengerId')
data_train.pop('PassengerId')
print(data_train)
print(data_test)

"""We can remove the columns Ticket and Cabin number because they are hard to calculate if they have NaN values."""

data_train = data_train.dropna(subset = ['Age'])

data_test = data_test.dropna(subset = ['Age'])

display(data_train)

"""Dropped all the Age values that are NaN to make the data cleaner and easier to read."""

def create_bar_graph_survival(column, nameOfGraph, axis):
  survivors = data_train[data_train['Survived'] == 1][column].value_counts()
  deceased = data_train[data_train['Survived'] == 0][column].value_counts()
  finalGraph = pd.DataFrame([survivors, deceased])
  finalGraph.index = ['Survivors', 'Deceased']
  finalGraph.plot(kind='bar', stacked=False, figsize=(10,7), grid=True, title=nameOfGraph, xlabel=axis, ylabel="Number of Passengers")

"""Made a bar graph method found on stackoverflow that will create bar graphs based off of the survivors and dead people."""

create_bar_graph('Pclass', 'Survivors and Deceased Based on Class', 'Pclass')

"""According to this bar graph, most of the deceased are from the third class, while the most survivors are from the first class. We can infer that people from the third class had a lower chance of surviving than the first class."""

create_bar_graph_survival('Embarked', 'Survivors and Deceased Based on Embarked', 'Embarked')

"""Based on this bar graph, most of the passengers are from Southhampton, while the least amount of passengers are from Queens.
1 = Cherbourg,
2 = Queens,
3 = Southhampton
"""

create_bar_graph_survival('Sex', 'Survivors and Deceased Based on Sex', 'Sex')

"""According to the bar graph, there were more female survivors and more male deceased."""

def create_bar_graph_class(column, nameOfGraph, axis):
  passengers = data_train[data_train['Embarked'] == 1][column].value_counts()
  passengers1 = data_train[data_train['Embarked'] == 2][column].value_counts()
  passengers2 = data_train[data_train['Embarked'] == 3][column].value_counts()
  finalGraph = pd.DataFrame([passengers, passengers1, passengers2])
  finalGraph.index = ['Cherbourg', 'Queenstown', 'Southampton']
  finalGraph.plot(kind='bar', stacked=False, figsize=(10,7), grid=True, title=nameOfGraph, xlabel=axis, ylabel="Number of Passengers")

"""Creating method to create bar graphs that show the amount of people in each class from where they embarked."""

create_bar_graph_class('Pclass', 'Passengers and Class', 'Pclass')

"""According to this graph, many passengers are from Southampton while the least amount of passengers are from Queenstown

Before modeling, have to convert everything to simple numerical values. First, we can bucket ages to have ranges.
"""

display(data_train)

data_train.loc[data_train['Age'] <= 18, 'Age'] = 0
data_train.loc[(data_train['Age'] > 18) & (data_train['Age'] <=36), 'Age'] = 1
data_train.loc[(data_train['Age'] > 36) & (data_train['Age'] <=54), 'Age'] = 2
data_train.loc[data_train['Age'] > 54, 'Age'] = 3
data_test.loc[data_test['Age'] <= 18, 'Age'] = 0
data_test.loc[(data_test['Age'] > 18) & (data_test['Age'] <=36), 'Age'] = 1
data_test.loc[(data_test['Age'] > 36) & (data_test['Age'] <=54), 'Age'] = 2
data_test.loc[data_test['Age'] > 54, 'Age'] = 3

display(data_train)

"""Now, we can bucket the fares as well to have ranges. Ranges are based off of real Titanic ticket prices"""

data_train.loc[data_train['Fare'] <= 7 , 'Fare'] = 0
data_train.loc[(data_train['Fare'] > 7) & (data_train['Fare'] <=12), 'Fare'] = 1
data_train.loc[(data_train['Fare'] > 12) & (data_train['Fare'] <= 30), 'Fare'] = 2
data_train.loc[data_train['Fare'] > 30, 'Fare'] = 3
data_test.loc[data_test['Fare'] <= 7 , 'Fare'] = 0
data_test.loc[(data_test['Fare'] > 7) & (data_test['Fare'] <=12), 'Fare'] = 1
data_test.loc[(data_test['Fare'] > 12) & (data_test['Fare'] <= 30), 'Fare'] = 2
data_test.loc[data_test['Fare'] > 30, 'Fare'] = 3

display(data_train)

"""Now, it is time for the modeling. I am starting with Support Vector Machine"""

X_train = data_train.drop("Survived", axis=1)
Y_train = data_train["Survived"]

k_fold = KFold(n_splits=5, shuffle=False, random_state=None)
def KNeighborsModel():
  accuracy = cross_val_score(KNeighborsClassifier(n_neighbors=20), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='accuracy')
  precision = cross_val_score(KNeighborsClassifier(n_neighbors=20), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='precision')
  recall = cross_val_score(KNeighborsClassifier(n_neighbors=20), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='recall')
  totalaccuracy = 0
  for i in accuracy:
    totalaccuracy += i
  totalaccuracy/=5
  totalprecision = 0
  for i in precision:
    totalprecision+=i
  totalprecision/=5
  totalrecall = 0
  for i in recall:
    totalrecall+=i
  totalrecall/=5
  print("KNeighbor model scores (Accuracy, Precision, and Recall in order): ", totalaccuracy, ", ", totalprecision, ", ", totalrecall)
KNeighborsModel()

def SVCModel():
  accuracy = cross_val_score(SVC(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='accuracy')
  precision = cross_val_score(SVC(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='precision')
  recall = cross_val_score(SVC(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='recall')
  totalaccuracy = 0
  for i in accuracy:
    totalaccuracy += i
  totalaccuracy/=5
  totalprecision = 0
  for i in precision:
    totalprecision+=i
  totalprecision/=5
  totalrecall = 0
  for i in recall:
    totalrecall+=i
  totalrecall/=5
  print("SVC model scores (Accuracy, Precision, and Recall in order): ", totalaccuracy, ", ", totalprecision, ", ", totalrecall)
SVCModel()

def DecisionTreeModel():
  accuracy = cross_val_score(DecisionTreeClassifier(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='accuracy')
  precision = cross_val_score(DecisionTreeClassifier(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='precision')
  recall = cross_val_score(DecisionTreeClassifier(), X_train, Y_train, cv=k_fold, n_jobs=1, scoring='recall')
  totalaccuracy=0
  for i in accuracy:
    totalaccuracy += i
  totalaccuracy/=5
  totalprecision = 0
  for i in precision:
    totalprecision+=i
  totalprecision/=5
  totalrecall = 0
  for i in recall:
    totalrecall+=i
  totalrecall/=5
  print("Decision Tree model scores (Accuracy, Precision, and Recall in order): ", totalaccuracy, ", ", totalprecision, ", ", totalrecall)
DecisionTreeModel()